[=== Module cudatoolkit/12.1.1 loaded ===]
[=== Module cuda/12.1.1/cudnn/8.9 loaded ===]
INFO:__main__:Random seed set to: 42
INFO:__main__:================================================================================
INFO:__main__:Unified Training Script
INFO:__main__:================================================================================
INFO:__main__:Model Type: sigmagpt
INFO:__main__:================================================================================
INFO:__main__:Training Configuration:
INFO:__main__:================================================================================
INFO:__main__:  adam_beta1: 0.9
INFO:__main__:  adam_beta2: 0.999
INFO:__main__:  adam_epsilon: 1e-08
INFO:__main__:  batch_size: 8
INFO:__main__:  cond_pct_max: 0.2
INFO:__main__:  cond_pct_min: 0.0
INFO:__main__:  conditioning_sampling: blockwise
INFO:__main__:  dataset_config: wikitext-103-raw-v1
INFO:__main__:  dataset_name: wikitext
INFO:__main__:  device: cuda
INFO:__main__:  do_eval: True
INFO:__main__:  early_stopping_patience: 0
INFO:__main__:  eval_batch_size: 16
INFO:__main__:  eval_pct_max: 1.0
INFO:__main__:  eval_pct_min: 1.0
INFO:__main__:  eval_steps: 100
INFO:__main__:  evaluation_sampling: blockwise
INFO:__main__:  exp_name: TEST_RESUME_cond0-20_rope_distilgpt2_sigmagpt_temporal_think_expectation_20260101_203403
INFO:__main__:  fp16: False
INFO:__main__:  gradient_accumulation_steps: 4
INFO:__main__:  gradient_checkpointing: False
INFO:__main__:  learning_rate: 0.0005
INFO:__main__:  logging_steps: 1
INFO:__main__:  max_cond_blocks: None
INFO:__main__:  max_eval_batches: 5
INFO:__main__:  max_grad_norm: 1.0
INFO:__main__:  min_lr_ratio: 0.1
INFO:__main__:  mode2_boundary_cond_pct_max: 0.2
INFO:__main__:  mode2_boundary_cond_pct_min: 0.0
INFO:__main__:  model_config: distilgpt2
INFO:__main__:  model_type: sigmagpt
INFO:__main__:  num_epochs: 3
INFO:__main__:  num_eval_samples: 100
INFO:__main__:  num_train_samples: 10000
INFO:__main__:  num_workers: 4
INFO:__main__:  ordering_mode: temporal
INFO:__main__:  output_dir: ./experiments
INFO:__main__:  position_encoding_type: rope
INFO:__main__:  primary_dataset_only: True
INFO:__main__:  resume_from: None
INFO:__main__:  rope_base: 10000.0
INFO:__main__:  save_steps: 5
INFO:__main__:  seed: 42
INFO:__main__:  sigmagpt_arch: new
INFO:__main__:  sigmagpt_eval_mode: autoregressive
INFO:__main__:  sigmagpt_mode: fair
INFO:__main__:  streaming: False
INFO:__main__:  thinking_token_mode: expectation
INFO:__main__:  use_thinking_tokens: True
INFO:__main__:  warmup_start_factor: 0.1
INFO:__main__:  warmup_steps: 50
INFO:__main__:  weight_decay: 0.1
INFO:__main__:================================================================================
INFO:__main__:Creating Sigma GPT Trainer...
INFO:train.base_trainer:Using device: cuda
INFO:train.base_trainer:Experiment directory: experiments/TEST_RESUME_cond0-20_rope_distilgpt2_sigmagpt_temporal_think_expectation_20260101_203403
INFO:train.base_trainer:Setting up model...
INFO:train.sigmagpt_trainer:Setting up Sigma GPT model...
INFO:train.sigmagpt_trainer:Position encoding type: dual_rope
INFO:model.thinking_tokens:Computed thinking token count: n=102 (cond_pct_max=0.2, mode=expectation)
INFO:train.sigmagpt_trainer:Thinking tokens: 102 (mode=expectation, cond_pct_max=0.2)
INFO:model.token_manager:Loading GPT-2 tokenizer...
INFO:model.token_manager:Added 102 thinking tokens: [think_1] (ID: 50257) to [think_102] (ID: 50358)
INFO:model.token_manager:TokenManager initialized:
INFO:model.token_manager:  Original vocab size: 50257
INFO:model.token_manager:  New vocab size: 50359
INFO:model.token_manager:  Mask token: None (ID: None)
INFO:model.token_manager:  BOS token: <|endoftext|> (ID: 50256)
INFO:model.token_manager:  Thinking tokens: 102 (IDs: 50257-50358)
INFO:train.sigmagpt_trainer:TokenManager created with 102 thinking tokens (IDs: 50257-50358)
INFO:model.thinking_tokens:ThinkingTokenPrepender initialized with 102 tokens (IDs: 50257 to 50358)
INFO:train.sigmagpt_trainer:Sigma GPT NEW (from baseline) initialized with 81.13M parameters
INFO:model.token_manager:Resizing embeddings from 50257 to 50359
INFO:model.token_manager:Embeddings resized successfully
INFO:model.token_manager:  Token embedding shape: torch.Size([50359, 768])
INFO:model.token_manager:  LM head shape: torch.Size([50359, 768])
INFO:train.sigmagpt_trainer:Model embeddings resized for thinking tokens
INFO:train.sigmagpt_trainer:Training mode: fair (~40% learning efficiency)
INFO:train.sigmagpt_trainer:Evaluation mode: autoregressive
INFO:train.base_trainer:Setting up data loaders...
INFO:train.sigmagpt_trainer:Setting up data components...
INFO:train.sigmagpt_trainer:Distribution config: cond=0%-20%, eval=100%-100%
INFO:train.augmentation:ConditionalAugmenter initialized:
INFO:train.augmentation:  Mask token ID: 50256
INFO:train.augmentation:  BOS token ID: 50256
INFO:train.augmentation:  Padding:
INFO:train.augmentation:    Max sequence length: 1024
INFO:train.augmentation:    Max conditioning %: 50.0%
INFO:train.augmentation:    Augmented max length: 1537
INFO:train.augmentation:  Mode: Distribution-based sampling
INFO:train.augmentation:  Conditioning sampling: blockwise
INFO:train.augmentation:  Evaluation sampling: blockwise
INFO:train.augmentation:  Ordering mode: temporal
INFO:train.sigmagpt_trainer:ConditionalAugmenter initialized (conditioning: blockwise, evaluation: blockwise, ordering: temporal)
INFO:train.sigmagpt_trainer:SigmaGPTDataAdapter initialized (mode: fair)
INFO:train.sigmagpt_trainer:Creating training dataloader (batch_size: 8, num_workers: 4)...
INFO:train.dataset:Loading GPT-2 tokenizer...
INFO:train.dataset:Loading Wikipedia dataset: wikitext (wikitext-103-raw-v1), split=train
INFO:train.dataset:Attempting to load dataset wikitext (attempt 1/3)...
INFO:train.dataset:Successfully loaded wikitext!
INFO:train.dataset:Using 10000 documents out of 1801350
INFO:train.dataset:Dataset loaded successfully. Documents: 10000
INFO:train.dataset:Preprocessing: concatenating documents and chunking...
INFO:train.dataset:Processing 10000 documents...
INFO:train.dataset:  Processed 10000/10000 documents, accumulated 649,057 tokens
INFO:train.dataset:Total tokens accumulated: 649,057
INFO:train.dataset:Created 633 complete chunks
INFO:train.dataset:Dropped 865 tokens (0.13%) from incomplete last chunk
INFO:train.dataset:Created 633 chunks of length 1024
INFO:train.sigmagpt_trainer:Training dataloader created with 80 batches per epoch
INFO:train.sigmagpt_trainer:Creating validation dataloader (batch_size: 16, num_workers: 4)...
INFO:train.dataset:Loading GPT-2 tokenizer...
INFO:train.dataset:Loading Wikipedia dataset: wikitext (wikitext-103-raw-v1), split=validation
INFO:train.dataset:Attempting to load dataset wikitext (attempt 1/3)...
INFO:train.dataset:Successfully loaded wikitext!
INFO:train.dataset:Using 100 documents out of 3760
INFO:train.dataset:Dataset loaded successfully. Documents: 100
INFO:train.dataset:Preprocessing: concatenating documents and chunking...
INFO:train.dataset:Processing 100 documents...
INFO:train.dataset:Total tokens accumulated: 6,351
INFO:train.dataset:Created 6 complete chunks
INFO:train.dataset:Dropped 207 tokens (3.26%) from incomplete last chunk
INFO:train.dataset:Created 6 chunks of length 1024
INFO:train.sigmagpt_trainer:Validation dataloader created with 1 batches
INFO:train.base_trainer:Setting up optimizer and scheduler...
INFO:train.base_trainer:Total training steps: 60
INFO:train.base_trainer:Warmup steps: 50
INFO:train.base_trainer:================================================================================
INFO:train.base_trainer:Starting Training
INFO:train.base_trainer:================================================================================
INFO:train.base_trainer:Model parameters: 81.20M
INFO:train.base_trainer:Training samples: 633
INFO:train.base_trainer:Batch size: 8
INFO:train.base_trainer:Gradient accumulation steps: 4
INFO:train.base_trainer:Effective batch size: 32
INFO:train.base_trainer:Number of epochs: 3
INFO:train.base_trainer:Total training steps: 60
INFO:train.base_trainer:================================================================================
INFO:train.base_trainer:
Epoch 1/3
Epoch 1:   0%|          | 0/80 [00:00<?, ?it/s]Epoch 1:   1%|▏         | 1/80 [00:01<01:39,  1.25s/it]Epoch 1:   2%|▎         | 2/80 [00:01<00:53,  1.47it/s]Epoch 1:   4%|▍         | 3/80 [00:01<00:37,  2.04it/s]Epoch 1:   4%|▍         | 3/80 [00:02<00:37,  2.04it/s, loss=10.9828, lr=5.90e-05]INFO:train.base_trainer:Step 1/60 | Epoch 1 | Loss: 10.9828 | PPL: 58854.09 | LR: 5.90e-05
Epoch 1:   5%|▌         | 4/80 [00:02<00:32,  2.33it/s, loss=10.9828, lr=5.90e-05]Epoch 1:   6%|▋         | 5/80 [00:02<00:27,  2.76it/s, loss=10.9828, lr=5.90e-05]Epoch 1:   8%|▊         | 6/80 [00:02<00:23,  3.10it/s, loss=10.9828, lr=5.90e-05]Epoch 1:   9%|▉         | 7/80 [00:02<00:22,  3.31it/s, loss=10.9828, lr=5.90e-05]Epoch 1:   9%|▉         | 7/80 [00:03<00:22,  3.31it/s, loss=10.3637, lr=6.80e-05]INFO:train.base_trainer:Step 2/60 | Epoch 1 | Loss: 10.3637 | PPL: 31688.18 | LR: 6.80e-05
Epoch 1:  10%|█         | 8/80 [00:03<00:24,  2.94it/s, loss=10.3637, lr=6.80e-05]Epoch 1:  11%|█▏        | 9/80 [00:03<00:22,  3.18it/s, loss=10.3637, lr=6.80e-05]Epoch 1:  12%|█▎        | 10/80 [00:03<00:20,  3.36it/s, loss=10.3637, lr=6.80e-05]Epoch 1:  14%|█▍        | 11/80 [00:04<00:19,  3.50it/s, loss=10.3637, lr=6.80e-05]Epoch 1:  14%|█▍        | 11/80 [00:04<00:19,  3.50it/s, loss=10.0758, lr=7.70e-05]INFO:train.base_trainer:Step 3/60 | Epoch 1 | Loss: 10.0758 | PPL: 23762.02 | LR: 7.70e-05
Epoch 1:  15%|█▌        | 12/80 [00:04<00:18,  3.59it/s, loss=10.0758, lr=7.70e-05]Epoch 1:  16%|█▋        | 13/80 [00:04<00:18,  3.66it/s, loss=10.0758, lr=7.70e-05]Epoch 1:  18%|█▊        | 14/80 [00:04<00:18,  3.65it/s, loss=10.0758, lr=7.70e-05]Epoch 1:  19%|█▉        | 15/80 [00:05<00:17,  3.71it/s, loss=10.0758, lr=7.70e-05]Epoch 1:  19%|█▉        | 15/80 [00:05<00:17,  3.71it/s, loss=9.8628, lr=8.60e-05] INFO:train.base_trainer:Step 4/60 | Epoch 1 | Loss: 9.8628 | PPL: 19202.06 | LR: 8.60e-05
Epoch 1:  20%|██        | 16/80 [00:05<00:16,  3.80it/s, loss=9.8628, lr=8.60e-05]Epoch 1:  21%|██▏       | 17/80 [00:05<00:16,  3.89it/s, loss=9.8628, lr=8.60e-05]Epoch 1:  22%|██▎       | 18/80 [00:05<00:15,  3.88it/s, loss=9.8628, lr=8.60e-05]Epoch 1:  24%|██▍       | 19/80 [00:06<00:15,  3.86it/s, loss=9.8628, lr=8.60e-05]Epoch 1:  24%|██▍       | 19/80 [00:06<00:15,  3.86it/s, loss=9.6748, lr=9.50e-05]INFO:train.base_trainer:Step 5/60 | Epoch 1 | Loss: 9.6748 | PPL: 15911.81 | LR: 9.50e-05
Epoch 1:  24%|██▍       | 19/80 [00:08<00:26,  2.33it/s, loss=9.6748, lr=9.50e-05]
ERROR:__main__:
================================================================================
ERROR:__main__:Training failed with error: name 'os' is not defined
ERROR:__main__:================================================================================
Traceback (most recent call last):
  File "/network/scratch/l/luy/Arbitrary Probability Model/./train.py", line 767, in <module>
    main()
  File "/network/scratch/l/luy/Arbitrary Probability Model/./train.py", line 751, in main
    trainer.train()
  File "/network/scratch/l/luy/Arbitrary Probability Model/train/base_trainer.py", line 648, in train
    self._save_checkpoint(f"checkpoint_step_{self.global_step}")
  File "/network/scratch/l/luy/Arbitrary Probability Model/train/base_trainer.py", line 735, in _save_checkpoint
    os.rename(temp_path, checkpoint_path)
NameError: name 'os' is not defined
/etc/slurm/slurmtask_epilog: line 25: [: too many arguments
