=========================================
TEST: Verifying eval bug in shell script
=========================================
[=== Module cudatoolkit/12.1.1 loaded ===]
[=== Module cuda/12.1.1/cudnn/8.9 loaded ===]
INFO:__main__:Random seed set to: 42
INFO:__main__:================================================================================
INFO:__main__:Unified Training Script
INFO:__main__:================================================================================
INFO:__main__:Model Type: distilbert
INFO:__main__:================================================================================
INFO:__main__:Training Configuration:
INFO:__main__:================================================================================
INFO:__main__:  adam_beta1: 0.9
INFO:__main__:  adam_beta2: 0.999
INFO:__main__:  adam_epsilon: 1e-08
INFO:__main__:  batch_size: 4
INFO:__main__:  cond_pct_max: 0.4
INFO:__main__:  cond_pct_min: 0.2
INFO:__main__:  dataset_config: wikitext-103-v1
INFO:__main__:  dataset_name: wikitext
INFO:__main__:  device: cuda
INFO:__main__:  do_eval: True
INFO:__main__:  early_stopping_patience: 0
INFO:__main__:  eval_batch_size: 4
INFO:__main__:  eval_steps: 10
INFO:__main__:  exp_name: test_eval_bug
INFO:__main__:  fp16: False
INFO:__main__:  gradient_accumulation_steps: 4
INFO:__main__:  learning_rate: 0.0005
INFO:__main__:  logging_steps: 5
INFO:__main__:  max_eval_batches: 2
INFO:__main__:  max_grad_norm: 1.0
INFO:__main__:  min_lr_ratio: 0.1
INFO:__main__:  model_config: distilbert-base-uncased
INFO:__main__:  model_type: distilbert
INFO:__main__:  num_epochs: 1
INFO:__main__:  num_eval_samples: 100
INFO:__main__:  num_train_samples: 1000
INFO:__main__:  num_workers: 2
INFO:__main__:  output_dir: ./experiments
INFO:__main__:  save_steps: 10000
INFO:__main__:  seed: 42
INFO:__main__:  streaming: False
INFO:__main__:  warmup_start_factor: 0.1
INFO:__main__:  warmup_steps: 2000
INFO:__main__:  weight_decay: 0.01
INFO:__main__:================================================================================
INFO:__main__:Creating DistilBert Trainer...
INFO:train.base_trainer:Using device: cuda
INFO:train.base_trainer:Experiment directory: experiments/test_eval_bug_20251211_200140
INFO:train.base_trainer:Setting up model...
INFO:train.distilbert_trainer:Setting up DistilBERT model...
INFO:train.distilbert_trainer:Tokenizer vocab size: 50258
Traceback (most recent call last):
  File "/network/scratch/l/luy/545_bert_test/./train.py", line 627, in <module>
    main()
  File "/network/scratch/l/luy/545_bert_test/./train.py", line 607, in main
    trainer = create_trainer(args)
  File "/network/scratch/l/luy/545_bert_test/./train.py", line 580, in create_trainer
    return DistilBertTrainer(args)
  File "/network/scratch/l/luy/545_bert_test/train/base_trainer.py", line 77, in __init__
    self.setup_model()
  File "/network/scratch/l/luy/545_bert_test/train/distilbert_trainer.py", line 51, in setup_model
    self.model = DistilBertForMaskedLM(self.config).to(self.device)
  File "/home/mila/l/luy/.conda/envs/arbprob/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1371, in to
    return self._apply(convert)
  File "/home/mila/l/luy/.conda/envs/arbprob/lib/python3.10/site-packages/torch/nn/modules/module.py", line 930, in _apply
    module._apply(fn)
  File "/home/mila/l/luy/.conda/envs/arbprob/lib/python3.10/site-packages/torch/nn/modules/module.py", line 957, in _apply
    param_applied = fn(param)
  File "/home/mila/l/luy/.conda/envs/arbprob/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1357, in convert
    return t.to(
  File "/home/mila/l/luy/.conda/envs/arbprob/lib/python3.10/site-packages/torch/cuda/__init__.py", line 410, in _lazy_init
    torch._C._cuda_init()
RuntimeError: Found no NVIDIA driver on your system. Please check that you have an NVIDIA GPU and installed a driver from http://www.nvidia.com/Download/index.aspx
=========================================
Test completed. Check if 'Running BERT multi-mode evaluation' appeared in logs.
=========================================
