{
  "model_config": "distilgpt2",
  "dataset_name": "wikipedia",
  "dataset_config": "20220301.en",
  "streaming": false,
  "num_train_samples": null,
  "num_eval_samples": 10000,
  "num_workers": 4,
  "num_epochs": 3,
  "max_steps": -1,
  "batch_size": 8,
  "eval_batch_size": 16,
  "gradient_accumulation_steps": 16,
  "learning_rate": 0.0005,
  "weight_decay": 0.01,
  "adam_beta1": 0.9,
  "adam_beta2": 0.999,
  "adam_epsilon": 1e-08,
  "max_grad_norm": 1.0,
  "warmup_steps": 2000,
  "warmup_start_factor": 0.1,
  "min_lr_ratio": 0.1,
  "fp16": true,
  "logging_steps": 10,
  "eval_steps": 500,
  "save_steps": 1000,
  "max_eval_batches": 100,
  "do_eval": true,
  "use_wandb": false,
  "wandb_project": "distilgpt2-wikipedia",
  "output_dir": "./experiments",
  "exp_name": "distilgpt2_wikipedia_full",
  "resume_from_checkpoint": null,
  "device": "cuda"
}